{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold, cross_val_score\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = r\"C:/Pro/Cours/A5 - IPP/DataCamp/Individual_Ramp_Challenge/Datacamp-Challenge-Volcanic-events-prediction-from-tephras/data/\"\n",
    "\n",
    "train = pd.read_csv(basedir + 'train_imputed.csv')\n",
    "test = pd.read_csv(basedir + 'test_imputed.csv')\n",
    "groups = pd.read_csv(basedir + 'train.csv')[\"SampleID\"].astype(\"category\").cat.codes.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(\"Event\", axis=1)\n",
    "y_train = train[\"Event\"]\n",
    "\n",
    "X_test = test.drop(\"Event\", axis=1)\n",
    "y_test = test[\"Event\"]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "majors = ['SiO2_normalized', 'TiO2_normalized', 'Al2O3_normalized',\n",
    "          'FeOT_normalized',\n",
    "          # 'FeO_normalized', 'Fe2O3_normalized', 'Fe2O3T_normalized',\n",
    "          'MnO_normalized', 'MgO_normalized', 'CaO_normalized',\n",
    "          'Na2O_normalized', 'K2O_normalized',\n",
    "          # 'P2O5_normalized','Cl_normalized'\n",
    "          ]\n",
    "traces = ['Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Cs', 'Ba', 'La',\n",
    "          'Ce', 'Pr', 'Nd', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy',\n",
    "          'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'Pb',\n",
    "          'Th', 'U']\n",
    "\n",
    "label = \"Event\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get column index of majors and traces\n",
    "\n",
    "majors_idx = [X_train.columns.get_loc(c) for c in majors if c in X_train]\n",
    "traces_idx = [X_train.columns.get_loc(c) for c in traces if c in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majors_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Classifier(BaseEstimator):\\n    def __init__(self):\\n        self.transformer = Pipeline(\\n            steps=[\\n                (\"imputer_majors\", IterativeImputer(random_state=0, estimator=BayesianRidge(), max_iter=10)),\\n                (\"imputer_traces\", IterativeImputer(random_state=0, estimator=SVR(kernel=\\'linear\\'), max_iter=10)),\\n                (\"scaler\", MinMaxScaler()),\\n            ]\\n        )\\n        self.model = KNeighborsClassifier(n_neighbors=5)\\n        self.pipe = make_pipeline(self.transformer, self.model)\\n\\n    def fit(self, X, y):\\n        self.pipe.fit(X, y)\\n\\n    def predict(self, X):\\n        return self.pipe.predict(X)\\n\\n    def predict_proba(self, X):\\n        return self.pipe.predict_proba(X)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.transformer = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer_majors\", IterativeImputer(random_state=0, estimator=BayesianRidge(), max_iter=10)),\n",
    "                (\"imputer_traces\", IterativeImputer(random_state=0, estimator=SVR(kernel='linear'), max_iter=10)),\n",
    "                (\"scaler\", MinMaxScaler()),\n",
    "            ]\n",
    "        )\n",
    "        self.model = KNeighborsClassifier(n_neighbors=5)\n",
    "        self.pipe = make_pipeline(self.transformer, self.model)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.pipe.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.pipe.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.pipe.predict_proba(X)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Classifier(BaseEstimator):\\n    def __init__(self):\\n        self.transformer = Pipeline([\\n            (\"impute_scale\", ColumnTransformer([\\n                (\"imputer_majors\", IterativeImputer(random_state=0, estimator=BayesianRidge(), max_iter=10), majors),\\n                (\"imputer_traces\", IterativeImputer(random_state=0, estimator=SVR(kernel=\\'linear\\'), max_iter=10), traces),\\n            ], remainder=\\'passthrough\\')),\\n            (\"scaler\", MinMaxScaler()),\\n        ])\\n        self.model = KNeighborsClassifier(n_neighbors=5)\\n        self.pipe = make_pipeline(self.transformer, self.model)\\n\\n    def fit(self, X, y):\\n        self.pipe.fit(X, y)\\n\\n    def predict(self, X):\\n        return self.pipe.predict(X)\\n\\n    def predict_proba(self, X):\\n        return self.pipe.predict_proba(X)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.transformer = Pipeline([\n",
    "            (\"impute_scale\", ColumnTransformer([\n",
    "                (\"imputer_majors\", IterativeImputer(random_state=0, estimator=BayesianRidge(), max_iter=10), majors),\n",
    "                (\"imputer_traces\", IterativeImputer(random_state=0, estimator=SVR(kernel='linear'), max_iter=10), traces),\n",
    "            ], remainder='passthrough')),\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "        ])\n",
    "        self.model = KNeighborsClassifier(n_neighbors=5)\n",
    "        self.pipe = make_pipeline(self.transformer, self.model)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.pipe.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.pipe.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.pipe.predict_proba(X)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.transformer = Pipeline([\n",
    "            (\"impute_scale\", ColumnTransformer([\n",
    "                (\"imputer_majors\", IterativeImputer(random_state=0, estimator=BayesianRidge(), max_iter=10), majors),\n",
    "                (\"imputer_traces\", SimpleImputer(strategy=\"median\"), traces),\n",
    "            ], remainder='passthrough')),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ])\n",
    "        self.model = HistGradientBoostingClassifier()\n",
    "        self.pipe = make_pipeline(self.transformer, self.model)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.pipe.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.pipe.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.pipe.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6220, 35)\n",
      "(839, 35)\n",
      "(6220,)\n",
      "(839,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Score:  0.8054194396841455 \n",
      "\n",
      "\n",
      "{'l2_regularization': 1.5, 'learning_rate': 0.1, 'max_depth': 25, 'max_iter': 1500, 'scoring': 'f1_micro'}\n"
     ]
    }
   ],
   "source": [
    "params = {'max_iter': [1000,1200,1500],\n",
    "          'learning_rate': [0.1],\n",
    "          'max_depth' : [25, 50, 75],\n",
    "          'l2_regularization': [1.5],\n",
    "          'scoring': ['f1_micro']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(HistGradientBoostingClassifier(),\n",
    "                  param_grid=params,\n",
    "                  cv=StratifiedGroupKFold(n_splits=2),\n",
    "                  scoring=\"balanced_accuracy\",\n",
    "                  verbose=1)\n",
    "\n",
    "gs.fit(X_train, y_train, groups=groups)\n",
    "print(\"Score: \", gs.score(X_test, y_test), \"\\n\\n\")\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5751250483243812"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Classifier()\n",
    "clf.fit(X_train, y_train)\n",
    "sf = StratifiedGroupKFold(n_splits=5)\n",
    "sf.split(X_train, y_train, groups)\n",
    "cross_val_score(clf, X_train, y_train, groups=groups, cv=sf, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Scripts\\ramp-test.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\rampwf\\utils\\cli\\testing.py\", line 117, in start\n",
      "    main()\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\click\\core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\click\\core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\click\\core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\rampwf\\utils\\cli\\testing.py\", line 102, in main\n",
      "    assert_submission(ramp_kit_dir=ramp_kit_dir,\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\rampwf\\utils\\testing.py\", line 103, in assert_submission\n",
      "    problem = assert_read_problem(ramp_kit_dir)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\rampwf\\utils\\testing.py\", line 33, in assert_read_problem\n",
      "    return import_module_from_source(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Pro\\Cours\\A5 - IPP\\DataCamp\\data_camp_venv\\Lib\\site-packages\\rampwf\\utils\\importing.py\", line 36, in import_module_from_source\n",
      "    spec.loader.exec_module(module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1073, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1130, in get_data\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'c:\\\\Pro\\\\Cours\\\\A5 - IPP\\\\DataCamp\\\\Individual_Ramp_Challenge\\\\Datacamp-Challenge-Volcanic-events-prediction-from-tephras\\\\Marius\\\\.\\\\problem.py'\n"
     ]
    }
   ],
   "source": [
    "!ramp-test --submission submissions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_camp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
